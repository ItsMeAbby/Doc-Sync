{
  "documents": [
    {
      "id": "d123c732-7f44-4cf0-aafb-13c5bb160bc6",
      "title": "Handoffs - OpenAI Agents SDK",
      "version": "8e83d392-da47-49eb-8bc8-27a1130e40c7",
      "markdown_content": "[Skip to content](https://openai.github.io/openai-agents-python/handoffs/#handoffs)\n\n# Handoffs\n\nHandoffs allow an agent to delegate tasks to another agent. This is particularly useful in scenarios where different agents specialize in distinct areas. For example, a customer support app might have agents that each specifically handle tasks like order status, refunds, FAQs, etc.\n\nHandoffs are represented as tools to the LLM. So if there's a handoff to an agent named `Refund Agent`, the tool would be called `transfer_to_refund_agent`.\n\n## Creating a handoff\n\nAll agents have a [`handoffs`](https://openai.github.io/openai-agents-python/ref/agent/#agents.agent.Agent.handoffs \"handoffs            class-attribute       instance-attribute   \") param, which can either take an `Agent` directly, or a `Handoff` object that customizes the Handoff.\n\nYou can create a handoff using the [`handoff()`](https://openai.github.io/openai-agents-python/ref/handoffs/#agents.handoffs.handoff \"handoff\") function provided by the Agents SDK. This function allows you to specify the agent to hand off to, along with optional overrides and input filters.\n\n### Basic Usage\n\nHere's how you can create a simple handoff:\n\n```md-code__content\nfrom agents import Agent, handoff\n\nbilling_agent = Agent(name=\"Billing agent\")\nrefund_agent = Agent(name=\"Refund agent\")\n\ntriage_agent = Agent(name=\"Triage agent\", handoffs=[billing_agent, handoff(refund_agent)])\n\n```\n\n### Customizing handoffs via the `handoff()` function\n\nThe [`handoff()`](https://openai.github.io/openai-agents-python/ref/handoffs/#agents.handoffs.handoff \"handoff\") function lets you customize things.\n\n- `agent`: This is the agent to which things will be handed off.\n- `tool_name_override`: By default, the `Handoff.default_tool_name()` function is used, which resolves to `transfer_to_<agent_name>`. You can override this.\n- `tool_description_override`: Override the default tool description from `Handoff.default_tool_description()`\n- `on_handoff`: A callback function executed when the handoff is invoked. This is useful for things like kicking off some data fetching as soon as you know a handoff is being invoked. This function receives the agent context, and can optionally also receive LLM generated input. The input data is controlled by the `input_type` param.\n- `input_type`: The type of input expected by the handoff (optional).\n- `input_filter`: This lets you filter the input received by the next agent. See below for more.\n\n```md-code__content\nfrom agents import Agent, handoff, RunContextWrapper\n\ndef on_handoff(ctx: RunContextWrapper[None]):\n    print(\"Handoff called\")\n\nagent = Agent(name=\"My agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    on_handoff=on_handoff,\n    tool_name_override=\"custom_handoff_tool\",\n    tool_description_override=\"Custom description\",\n)\n\n```\n\n## Handoff inputs\n\nIn certain situations, you want the LLM to provide some data when it calls a handoff. For example, imagine a handoff to an \"Escalation agent\". You might want a reason to be provided, so you can log it.\n\n```md-code__content\nfrom pydantic import BaseModel\n\nfrom agents import Agent, handoff, RunContextWrapper\n\nclass EscalationData(BaseModel):\n    reason: str\n\nasync def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):\n    print(f\"Escalation agent called with reason: {input_data.reason}\")\n\nagent = Agent(name=\"Escalation agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    on_handoff=on_handoff,\n    input_type=EscalationData,\n)\n\n```\n\n## Input filters\n\nWhen a handoff occurs, it's as though the new agent takes over the conversation, and gets to see the entire previous conversation history. If you want to change this, you can set an [`input_filter`](https://openai.github.io/openai-agents-python/ref/handoffs/#agents.handoffs.Handoff.input_filter \"input_filter            class-attribute       instance-attribute   \"). An input filter is a function that receives the existing input via a [`HandoffInputData`](https://openai.github.io/openai-agents-python/ref/handoffs/#agents.handoffs.HandoffInputData \"HandoffInputData            dataclass   \"), and must return a new `HandoffInputData`.\n\nThere are some common patterns (for example removing all tool calls from the history), which are implemented for you in [`agents.extensions.handoff_filters`](https://openai.github.io/openai-agents-python/ref/extensions/handoff_filters/#agents.extensions.handoff_filters)\n\n```md-code__content\nfrom agents import Agent, handoff\nfrom agents.extensions import handoff_filters\n\nagent = Agent(name=\"FAQ agent\")\n\nhandoff_obj = handoff(\n    agent=agent,\n    input_filter=handoff_filters.remove_all_tools,\n)\n\n```\n\n## Recommended prompts\n\nTo make sure that LLMs understand handoffs properly, we recommend including information about handoffs in your agents. We have a suggested prefix in [`agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX`](https://openai.github.io/openai-agents-python/ref/extensions/handoff_prompt/#agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX \"RECOMMENDED_PROMPT_PREFIX            module-attribute   \"), or you can call [`agents.extensions.handoff_prompt.prompt_with_handoff_instructions`](https://openai.github.io/openai-agents-python/ref/extensions/handoff_prompt/#agents.extensions.handoff_prompt.prompt_with_handoff_instructions \"prompt_with_handoff_instructions\") to automatically add recommended data to your prompts.\n\n```md-code__content\nfrom agents import Agent\nfrom agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX\n\nbilling_agent = Agent(\n    name=\"Billing agent\",\n    instructions=f\"\"\"{RECOMMENDED_PROMPT_PREFIX}\n    <Fill in the rest of your prompt here>.\"\"\",\n)\n\n```",
      "summary": "This documentation page explains how to implement handoffs between agents in the OpenAI Agents SDK. Handoffs let one agent delegate tasks to another, supporting customization, input handling, and input filtering. Guidance is provided on creation, customization, handling handoff data, and recommending prompt structures for LLM compatibility.",
      "similarity": 0.534799726891502,
      "path": "handoffs/",
      "language": "en",
      "keywords_array": [
        "handoff",
        "Agent",
        "handoffs parameter",
        "handoff() function",
        "input_filter",
        "RunContextWrapper",
        "HandoffInputData",
        "handoff_filters",
        "RECOMMENDED_PROMPT_PREFIX",
        "prompt_with_handoff_instructions"
      ],
      "urls_array": [
        "https://openai.github.io/openai-agents-python/ref/agent/#agents.agent.Agent.handoffs",
        "https://openai.github.io/openai-agents-python/ref/extensions/handoff_filters/#agents.extensions.handoff_filters",
        "https://openai.github.io/openai-agents-python/ref/handoffs/#agents.handoffs.Handoff.input_filter",
        "https://openai.github.io/openai-agents-python/ref/handoffs/#agents.handoffs.handoff",
        "https://openai.github.io/openai-agents-python/ref/extensions/handoff_prompt/#agents.extensions.handoff_prompt.prompt_with_handoff_instructions",
        "https://openai.github.io/openai-agents-python/ref/handoffs/#agents.handoffs.HandoffInputData",
        "https://openai.github.io/openai-agents-python/ref/extensions/handoff_prompt/#agents.extensions.handoff_prompt.RECOMMENDED_PROMPT_PREFIX",
        "https://openai.github.io/openai-agents-python/handoffs/#handoffs"
      ],
      "is_api_ref": false
    },
    {
      "id": "711a9ea9-d76a-4f67-98f4-4de5c3394a1e",
      "title": "Tools - OpenAI Agents SDK",
      "version": "3c294470-2ad6-4afa-b15e-4eee1e62e3c8",
      "markdown_content": "[Skip to content](https://openai.github.io/openai-agents-python/tools/#tools)\n\n# Tools\n\nTools let agents take actions: things like fetching data, running code, calling external APIs, and even using a computer. There are three classes of tools in the Agent SDK:\n\n- Hosted tools: these run on LLM servers alongside the AI models. OpenAI offers retrieval, web search and computer use as hosted tools.\n- Function calling: these allow you to use any Python function as a tool.\n- Agents as tools: this allows you to use an agent as a tool, allowing Agents to call other agents without handing off to them.\n\n## Hosted tools\n\nOpenAI offers a few built-in tools when using the [`OpenAIResponsesModel`](https://openai.github.io/openai-agents-python/ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel \"OpenAIResponsesModel\"):\n\n- The [`WebSearchTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.WebSearchTool \"WebSearchTool            dataclass   \") lets an agent search the web.\n- The [`FileSearchTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.FileSearchTool \"FileSearchTool            dataclass   \") allows retrieving information from your OpenAI Vector Stores.\n- The [`ComputerTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.ComputerTool \"ComputerTool            dataclass   \") allows automating computer use tasks.\n- The [`CodeInterpreterTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.CodeInterpreterTool \"CodeInterpreterTool            dataclass   \") lets the LLM execute code in a sandboxed environment.\n- The [`HostedMCPTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.HostedMCPTool \"HostedMCPTool            dataclass   \") exposes a remote MCP server's tools to the model.\n- The [`ImageGenerationTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.ImageGenerationTool \"ImageGenerationTool            dataclass   \") generates images from a prompt.\n- The [`LocalShellTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.LocalShellTool \"LocalShellTool            dataclass   \") runs shell commands on your machine.\n\n```md-code__content\nfrom agents import Agent, FileSearchTool, Runner, WebSearchTool\n\nagent = Agent(\n    name=\"Assistant\",\n    tools=[\\\n        WebSearchTool(),\\\n        FileSearchTool(\\\n            max_num_results=3,\\\n            vector_store_ids=[\"VECTOR_STORE_ID\"],\\\n        ),\\\n    ],\n)\n\nasync def main():\n    result = await Runner.run(agent, \"Which coffee shop should I go to, taking into account my preferences and the weather today in SF?\")\n    print(result.final_output)\n\n```\n\n## Function tools\n\nYou can use any Python function as a tool. The Agents SDK will setup the tool automatically:\n\n- The name of the tool will be the name of the Python function (or you can provide a name)\n- Tool description will be taken from the docstring of the function (or you can provide a description)\n- The schema for the function inputs is automatically created from the function's arguments\n- Descriptions for each input are taken from the docstring of the function, unless disabled\n\nWe use Python's `inspect` module to extract the function signature, along with [`griffe`](https://mkdocstrings.github.io/griffe/) to parse docstrings and `pydantic` for schema creation.\n\n```md-code__content\nimport json\n\nfrom typing_extensions import TypedDict, Any\n\nfrom agents import Agent, FunctionTool, RunContextWrapper, function_tool\n\nclass Location(TypedDict):\n    lat: float\n    long: float\n\n@function_tool\nasync def fetch_weather(location: Location) -> str:\n\n    \"\"\"Fetch the weather for a given location.\n\n    Args:\n        location: The location to fetch the weather for.\n    \"\"\"\n    # In real life, we'd fetch the weather from a weather API\n    return \"sunny\"\n\n@function_tool(name_override=\"fetch_data\")\ndef read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:\n    \"\"\"Read the contents of a file.\n\n    Args:\n        path: The path to the file to read.\n        directory: The directory to read the file from.\n    \"\"\"\n    # In real life, we'd read the file from the file system\n    return \"<file contents>\"\n\nagent = Agent(\n    name=\"Assistant\",\n    tools=[fetch_weather, read_file],\n)\n\nfor tool in agent.tools:\n    if isinstance(tool, FunctionTool):\n        print(tool.name)\n        print(tool.description)\n        print(json.dumps(tool.params_json_schema, indent=2))\n        print()\n\n```\n\nExpand to see output\n\n```md-code__content\nfetch_weather\nFetch the weather for a given location.\n{\n\"$defs\": {\n  \"Location\": {\n    \"properties\": {\n      \"lat\": {\n        \"title\": \"Lat\",\n        \"type\": \"number\"\n      },\n      \"long\": {\n        \"title\": \"Long\",\n        \"type\": \"number\"\n      }\n    },\n    \"required\": [\\\n      \"lat\",\\\n      \"long\"\\\n    ],\n    \"title\": \"Location\",\n    \"type\": \"object\"\n  }\n},\n\"properties\": {\n  \"location\": {\n    \"$ref\": \"#/$defs/Location\",\n    \"description\": \"The location to fetch the weather for.\"\n  }\n},\n\"required\": [\\\n  \"location\"\\\n],\n\"title\": \"fetch_weather_args\",\n\"type\": \"object\"\n}\n\nfetch_data\nRead the contents of a file.\n{\n\"properties\": {\n  \"path\": {\n    \"description\": \"The path to the file to read.\",\n    \"title\": \"Path\",\n    \"type\": \"string\"\n  },\n  \"directory\": {\n    \"anyOf\": [\\\n      {\\\n        \"type\": \"string\"\\\n      },\\\n      {\\\n        \"type\": \"null\"\\\n      }\\\n    ],\n    \"default\": null,\n    \"description\": \"The directory to read the file from.\",\n    \"title\": \"Directory\"\n  }\n},\n\"required\": [\\\n  \"path\"\\\n],\n\"title\": \"fetch_data_args\",\n\"type\": \"object\"\n}\n\n```\n\n### Custom function tools\n\nSometimes, you don't want to use a Python function as a tool. You can directly create a [`FunctionTool`](https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.FunctionTool \"FunctionTool            dataclass   \") if you prefer. You'll need to provide:\n\n- `name`\n- `description`\n- `params_json_schema`, which is the JSON schema for the arguments\n- `on_invoke_tool`, which is an async function that receives the context and the arguments as a JSON string, and must return the tool output as a string.\n\n```md-code__content\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom agents import RunContextWrapper, FunctionTool\n\ndef do_some_work(data: str) -> str:\n    return \"done\"\n\nclass FunctionArgs(BaseModel):\n    username: str\n    age: int\n\nasync def run_function(ctx: RunContextWrapper[Any], args: str) -> str:\n    parsed = FunctionArgs.model_validate_json(args)\n    return do_some_work(data=f\"{parsed.username} is {parsed.age} years old\")\n\ntool = FunctionTool(\n    name=\"process_user\",\n    description=\"Processes extracted user data\",\n    params_json_schema=FunctionArgs.model_json_schema(),\n    on_invoke_tool=run_function,\n)\n\n```\n\n### Automatic argument and docstring parsing\n\nAs mentioned before, we automatically parse the function signature to extract the schema for the tool, and we parse the docstring to extract descriptions for the tool and for individual arguments. Some notes on that:\n\n1. The signature parsing is done via the `inspect` module. We use type annotations to understand the types for the arguments, and dynamically build a Pydantic model to represent the overall schema. It supports most types, including Python primitives, Pydantic models, TypedDicts, and more.\n2. We use `griffe` to parse docstrings. Supported docstring formats are `google`, `sphinx` and `numpy`. We attempt to automatically detect the docstring format, but this is best-effort and you can explicitly set it when calling `function_tool`. You can also disable docstring parsing by setting `use_docstring_info` to `False`.\n\nThe code for the schema extraction lives in [`agents.function_schema`](https://openai.github.io/openai-agents-python/ref/function_schema/#agents.function_schema).\n\n## Agents as tools\n\nIn some workflows, you may want a central agent to orchestrate a network of specialized agents, instead of handing off control. You can do this by modeling agents as tools.\n\n```md-code__content\nfrom agents import Agent, Runner\nimport asyncio\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You translate the user's message to Spanish\",\n)\n\nfrench_agent = Agent(\n    name=\"French agent\",\n    instructions=\"You translate the user's message to French\",\n)\n\norchestrator_agent = Agent(\n    name=\"orchestrator_agent\",\n    instructions=(\n        \"You are a translation agent. You use the tools given to you to translate.\"\n        \"If asked for multiple translations, you call the relevant tools.\"\n    ),\n    tools=[\\\n        spanish_agent.as_tool(\\\n            tool_name=\"translate_to_spanish\",\\\n            tool_description=\"Translate the user's message to Spanish\",\\\n        ),\\\n        french_agent.as_tool(\\\n            tool_name=\"translate_to_french\",\\\n            tool_description=\"Translate the user's message to French\",\\\n        ),\\\n    ],\n)\n\nasync def main():\n    result = await Runner.run(orchestrator_agent, input=\"Say 'Hello, how are you?' in Spanish.\")\n    print(result.final_output)\n\n```\n\n### Customizing tool-agents\n\nThe `agent.as_tool` function is a convenience method to make it easy to turn an agent into a tool. It doesn't support all configuration though; for example, you can't set `max_turns`. For advanced use cases, use `Runner.run` directly in your tool implementation:\n\n```md-code__content\n@function_tool\nasync def run_my_agent() -> str:\n    \"\"\"A tool that runs the agent with custom configs\"\"\"\n\n    agent = Agent(name=\"My agent\", instructions=\"...\")\n\n    result = await Runner.run(\n        agent,\n        input=\"...\",\n        max_turns=5,\n        run_config=...\n    )\n\n    return str(result.final_output)\n\n```\n\n### Custom output extraction\n\nIn certain cases, you might want to modify the output of the tool-agents before returning it to the central agent. This may be useful if you want to:\n\n- Extract a specific piece of information (e.g., a JSON payload) from the sub-agent's chat history.\n- Convert or reformat the agent’s final answer (e.g., transform Markdown into plain text or CSV).\n- Validate the output or provide a fallback value when the agent’s response is missing or malformed.\n\nYou can do this by supplying the `custom_output_extractor` argument to the `as_tool` method:\n\n```md-code__content\nasync def extract_json_payload(run_result: RunResult) -> str:\n    # Scan the agent’s outputs in reverse order until we find a JSON-like message from a tool call.\n    for item in reversed(run_result.new_items):\n        if isinstance(item, ToolCallOutputItem) and item.output.strip().startswith(\"{\"):\n            return item.output.strip()\n    # Fallback to an empty JSON object if nothing was found\n    return \"{}\"\n\njson_tool = data_agent.as_tool(\n    tool_name=\"get_data_json\",\n    tool_description=\"Run the data agent and return only its JSON payload\",\n    custom_output_extractor=extract_json_payload,\n)\n\n```\n\n## Handling errors in function tools\n\nWhen you create a function tool via `@function_tool`, you can pass a `failure_error_function`. This is a function that provides an error response to the LLM in case the tool call crashes.\n\n- By default (i.e. if you don't pass anything), it runs a `default_tool_error_function` which tells the LLM an error occurred.\n- If you pass your own error function, it runs that instead, and sends the response to the LLM.\n- If you explicitly pass `None`, then any tool call errors will be re-raised for you to handle. This could be a `ModelBehaviorError` if the model produced invalid JSON, or a `UserError` if your code crashed, etc.\n\nIf you are manually creating a `FunctionTool` object, then you must handle errors inside the `on_invoke_tool` function.",
      "summary": "This documentation page explains how to use \"tools\" in the OpenAI Agents Python SDK. It covers three classes of tools: hosted tools (built-in by OpenAI, e.g., web search, code execution), custom function tools (based on Python functions), and using agents as tools for complex workflows. Guidance is provided for setup, customization, and error handling.",
      "similarity": 0.480289044345735,
      "path": "tools/",
      "language": "en",
      "keywords_array": [
        "FunctionTool",
        "Agent SDK",
        "Hosted tools",
        "OpenAIResponsesModel",
        "WebSearchTool",
        "FileSearchTool",
        "CodeInterpreterTool",
        "Pydantic",
        "function_tool decorator",
        "custom_output_extractor"
      ],
      "urls_array": [
        "https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.ComputerTool",
        "https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.FunctionTool",
        "https://openai.github.io/openai-agents-python/ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel",
        "https://openai.github.io/openai-agents-python/ref/function_schema/#agents.function_schema",
        "https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.HostedMCPTool",
        "https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.FileSearchTool",
        "https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.CodeInterpreterTool",
        "https://mkdocstrings.github.io/griffe/",
        "https://openai.github.io/openai-agents-python/tools/#tools",
        "https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.WebSearchTool",
        "https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.ImageGenerationTool",
        "https://openai.github.io/openai-agents-python/ref/tool/#agents.tool.LocalShellTool"
      ],
      "is_api_ref": false
    },
    {
      "id": "a1d44224-d450-4278-861e-50878445f82b",
      "title": "Agents - OpenAI Agents SDK",
      "version": "39c34059-97d3-488f-ab79-eca7dbd5fba4",
      "markdown_content": "[Skip to content](https://openai.github.io/openai-agents-python/agents/#agents)\n\n# Agents\n\nAgents are the core building block in your apps. An agent is a large language model (LLM), configured with instructions and tools.\n\n## Basic configuration\n\nThe most common properties of an agent you'll configure are:\n\n- `instructions`: also known as a developer message or system prompt.\n- `model`: which LLM to use, and optional `model_settings` to configure model tuning parameters like temperature, top\\_p, etc.\n- `tools`: Tools that the agent can use to achieve its tasks.\n\n```md-code__content\nfrom agents import Agent, ModelSettings, function_tool\n\n@function_tool\ndef get_weather(city: str) -> str:\n    return f\"The weather in {city} is sunny\"\n\nagent = Agent(\n    name=\"Haiku agent\",\n    instructions=\"Always respond in haiku form\",\n    model=\"o3-mini\",\n    tools=[get_weather],\n)\n\n```\n\n## Context\n\nAgents are generic on their `context` type. Context is a dependency-injection tool: it's an object you create and pass to `Runner.run()`, that is passed to every agent, tool, handoff etc, and it serves as a grab bag of dependencies and state for the agent run. You can provide any Python object as the context.\n\n```md-code__content\n@dataclass\nclass UserContext:\n    uid: str\n    is_pro_user: bool\n\n    async def fetch_purchases() -> list[Purchase]:\n        return ...\n\nagent = Agent[UserContext](\n    ...,\n)\n\n```\n\n## Output types\n\nBy default, agents produce plain text (i.e. `str`) outputs. If you want the agent to produce a particular type of output, you can use the `output_type` parameter. A common choice is to use [Pydantic](https://docs.pydantic.dev/) objects, but we support any type that can be wrapped in a Pydantic [TypeAdapter](https://docs.pydantic.dev/latest/api/type_adapter/) \\- dataclasses, lists, TypedDict, etc.\n\n```md-code__content\nfrom pydantic import BaseModel\nfrom agents import Agent\n\nclass CalendarEvent(BaseModel):\n    name: str\n    date: str\n    participants: list[str]\n\nagent = Agent(\n    name=\"Calendar extractor\",\n    instructions=\"Extract calendar events from text\",\n    output_type=CalendarEvent,\n)\n\n```\n\nNote\n\nWhen you pass an `output_type`, that tells the model to use [structured outputs](https://platform.openai.com/docs/guides/structured-outputs) instead of regular plain text responses.\n\n## Handoffs\n\nHandoffs are sub-agents that the agent can delegate to. You provide a list of handoffs, and the agent can choose to delegate to them if relevant. This is a powerful pattern that allows orchestrating modular, specialized agents that excel at a single task. Read more in the [handoffs](https://openai.github.io/openai-agents-python/handoffs/) documentation.\n\n```md-code__content\nfrom agents import Agent\n\nbooking_agent = Agent(...)\nrefund_agent = Agent(...)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=(\n        \"Help the user with their questions.\"\n        \"If they ask about booking, handoff to the booking agent.\"\n        \"If they ask about refunds, handoff to the refund agent.\"\n    ),\n    handoffs=[booking_agent, refund_agent],\n)\n\n```\n\n## Dynamic instructions\n\nIn most cases, you can provide instructions when you create the agent. However, you can also provide dynamic instructions via a function. The function will receive the agent and context, and must return the prompt. Both regular and `async` functions are accepted.\n\n```md-code__content\ndef dynamic_instructions(\n    context: RunContextWrapper[UserContext], agent: Agent[UserContext]\n) -> str:\n    return f\"The user's name is {context.context.name}. Help them with their questions.\"\n\nagent = Agent[UserContext](\n    name=\"Triage agent\",\n    instructions=dynamic_instructions,\n)\n\n```\n\n## Lifecycle events (hooks)\n\nSometimes, you want to observe the lifecycle of an agent. For example, you may want to log events, or pre-fetch data when certain events occur. You can hook into the agent lifecycle with the `hooks` property. Subclass the [`AgentHooks`](https://openai.github.io/openai-agents-python/ref/lifecycle/#agents.lifecycle.AgentHooks \"AgentHooks\") class, and override the methods you're interested in.\n\n## Guardrails\n\nGuardrails allow you to run checks/validations on user input, in parallel to the agent running. For example, you could screen the user's input for relevance. Read more in the [guardrails](https://openai.github.io/openai-agents-python/guardrails/) documentation.\n\n## Cloning/copying agents\n\nBy using the `clone()` method on an agent, you can duplicate an Agent, and optionally change any properties you like.\n\n```md-code__content\npirate_agent = Agent(\n    name=\"Pirate\",\n    instructions=\"Write like a pirate\",\n    model=\"o3-mini\",\n)\n\nrobot_agent = pirate_agent.clone(\n    name=\"Robot\",\n    instructions=\"Write like a robot\",\n)\n\n```\n\n## Forcing tool use\n\nSupplying a list of tools doesn't always mean the LLM will use a tool. You can force tool use by setting [`ModelSettings.tool_choice`](https://openai.github.io/openai-agents-python/ref/model_settings/#agents.model_settings.ModelSettings.tool_choice \"tool_choice            class-attribute       instance-attribute   \"). Valid values are:\n\n1. `auto`, which allows the LLM to decide whether or not to use a tool.\n2. `required`, which requires the LLM to use a tool (but it can intelligently decide which tool).\n3. `none`, which requires the LLM to _not_ use a tool.\n4. Setting a specific string e.g. `my_tool`, which requires the LLM to use that specific tool.\n\nNote\n\nTo prevent infinite loops, the framework automatically resets `tool_choice` to \"auto\" after a tool call. This behavior is configurable via [`agent.reset_tool_choice`](https://openai.github.io/openai-agents-python/ref/agent/#agents.agent.Agent.reset_tool_choice \"reset_tool_choice            class-attribute       instance-attribute   \"). The infinite loop is because tool results are sent to the LLM, which then generates another tool call because of `tool_choice`, ad infinitum.\n\nIf you want the Agent to completely stop after a tool call (rather than continuing with auto mode), you can set \\[ `Agent.tool_use_behavior=\"stop_on_first_tool\"`\\] which will directly use the tool output as the final response without further LLM processing.",
      "summary": "This documentation page explains how to configure and use \"agents\"—language model-based components—in your applications. It covers agent setup, context handling, output types, delegation with handoffs, dynamic instructions, lifecycle hooks, guardrails, cloning agents, and controlling tool usage.",
      "similarity": 0.434330262621613,
      "path": "agents/",
      "language": "en",
      "keywords_array": [
        "Agent",
        "Large Language Model (LLM)",
        "ModelSettings",
        "function_tool",
        "context",
        "output_type",
        "Pydantic",
        "handoffs",
        "AgentHooks",
        "guardrails"
      ],
      "urls_array": [
        "https://openai.github.io/openai-agents-python/ref/model_settings/#agents.model_settings.ModelSettings.tool_choice",
        "https://docs.pydantic.dev/latest/api/type_adapter/",
        "https://openai.github.io/openai-agents-python/ref/lifecycle/#agents.lifecycle.AgentHooks",
        "https://platform.openai.com/docs/guides/structured-outputs",
        "https://openai.github.io/openai-agents-python/ref/agent/#agents.agent.Agent.reset_tool_choice",
        "https://openai.github.io/openai-agents-python/agents/#agents",
        "https://docs.pydantic.dev/",
        "https://openai.github.io/openai-agents-python/handoffs/",
        "https://openai.github.io/openai-agents-python/guardrails/"
      ],
      "is_api_ref": false
    },
    {
      "id": "60f60293-89c0-4ee0-aae2-d2f8dff25242",
      "title": "Running agents - OpenAI Agents SDK",
      "version": "89c9ac2a-1b0b-4d1a-90b2-db3197dcae9f",
      "markdown_content": "[Skip to content](https://openai.github.io/openai-agents-python/running_agents/#running-agents)\n\n# Running agents\n\nYou can run agents via the [`Runner`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.Runner \"Runner\") class. You have 3 options:\n\n1. [`Runner.run()`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.Runner.run \"run            async       classmethod   \"), which runs async and returns a [`RunResult`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResult \"RunResult            dataclass   \").\n2. [`Runner.run_sync()`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.Runner.run_sync \"run_sync            classmethod   \"), which is a sync method and just runs `.run()` under the hood.\n3. [`Runner.run_streamed()`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.Runner.run_streamed \"run_streamed            classmethod   \"), which runs async and returns a [`RunResultStreaming`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultStreaming \"RunResultStreaming            dataclass   \"). It calls the LLM in streaming mode, and streams those events to you as they are received.\n\n```md-code__content\nfrom agents import Agent, Runner\n\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\")\n\n    result = await Runner.run(agent, \"Write a haiku about recursion in programming.\")\n    print(result.final_output)\n    # Code within the code,\n    # Functions calling themselves,\n    # Infinite loop's dance.\n\n```\n\nRead more in the [results guide](https://openai.github.io/openai-agents-python/results/).\n\n## The agent loop\n\nWhen you use the run method in `Runner`, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API.\n\nThe runner then runs a loop:\n\n1. We call the LLM for the current agent, with the current input.\n2. The LLM produces its output.\n1. If the LLM returns a `final_output`, the loop ends and we return the result.\n2. If the LLM does a handoff, we update the current agent and input, and re-run the loop.\n3. If the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop.\n3. If we exceed the `max_turns` passed, we raise a [`MaxTurnsExceeded`](https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.MaxTurnsExceeded \"MaxTurnsExceeded\") exception.\n\nNote\n\nThe rule for whether the LLM output is considered as a \"final output\" is that it produces text output with the desired type, and there are no tool calls.\n\n## Streaming\n\nStreaming allows you to additionally receive streaming events as the LLM runs. Once the stream is done, the [`RunResultStreaming`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultStreaming \"RunResultStreaming            dataclass   \") will contain the complete information about the run, including all the new outputs produces. You can call `.stream_events()` for the streaming events. Read more in the [streaming guide](https://openai.github.io/openai-agents-python/streaming/).\n\n## Run config\n\nThe `run_config` parameter lets you configure some global settings for the agent run:\n\n- [`model`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.model \"model            class-attribute       instance-attribute   \"): Allows setting a global LLM model to use, irrespective of what `model` each Agent has.\n- [`model_provider`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.model_provider \"model_provider            class-attribute       instance-attribute   \"): A model provider for looking up model names, which defaults to OpenAI.\n- [`model_settings`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.model_settings \"model_settings            class-attribute       instance-attribute   \"): Overrides agent-specific settings. For example, you can set a global `temperature` or `top_p`.\n- [`input_guardrails`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.input_guardrails \"input_guardrails            class-attribute       instance-attribute   \"), [`output_guardrails`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.output_guardrails \"output_guardrails            class-attribute       instance-attribute   \"): A list of input or output guardrails to include on all runs.\n- [`handoff_input_filter`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.handoff_input_filter \"handoff_input_filter            class-attribute       instance-attribute   \"): A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in [`Handoff.input_filter`](https://openai.github.io/openai-agents-python/ref/handoffs/#agents.handoffs.Handoff.input_filter \"input_filter            class-attribute       instance-attribute   \") for more details.\n- [`tracing_disabled`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.tracing_disabled \"tracing_disabled            class-attribute       instance-attribute   \"): Allows you to disable [tracing](https://openai.github.io/openai-agents-python/tracing/) for the entire run.\n- [`trace_include_sensitive_data`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.trace_include_sensitive_data \"trace_include_sensitive_data            class-attribute       instance-attribute   \"): Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.\n- [`workflow_name`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.workflow_name \"workflow_name            class-attribute       instance-attribute   \"), [`trace_id`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.trace_id \"trace_id            class-attribute       instance-attribute   \"), [`group_id`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.group_id \"group_id            class-attribute       instance-attribute   \"): Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting `workflow_name`. The group ID is an optional field that lets you link traces across multiple runs.\n- [`trace_metadata`](https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.trace_metadata \"trace_metadata            class-attribute       instance-attribute   \"): Metadata to include on all traces.\n\n## Conversations/chat threads\n\nCalling any of the run methods can result in one or more agents running (and hence one or more LLM calls), but it represents a single logical turn in a chat conversation. For example:\n\n1. User turn: user enter text\n2. Runner run: first agent calls LLM, runs tools, does a handoff to a second agent, second agent runs more tools, and then produces an output.\n\nAt the end of the agent run, you can choose what to show to the user. For example, you might show the user every new item generated by the agents, or just the final output. Either way, the user might then ask a followup question, in which case you can call the run method again.\n\nYou can use the base [`RunResultBase.to_input_list()`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.to_input_list \"to_input_list\") method to get the inputs for the next turn.\n\n```md-code__content\nasync def main():\n    agent = Agent(name=\"Assistant\", instructions=\"Reply very concisely.\")\n\n    with trace(workflow_name=\"Conversation\", group_id=thread_id):\n        # First turn\n        result = await Runner.run(agent, \"What city is the Golden Gate Bridge in?\")\n        print(result.final_output)\n        # San Francisco\n\n        # Second turn\n        new_input = result.to_input_list() + [{\"role\": \"user\", \"content\": \"What state is it in?\"}]\n        result = await Runner.run(agent, new_input)\n        print(result.final_output)\n        # California\n\n```\n\n## Exceptions\n\nThe SDK raises exceptions in certain cases. The full list is in [`agents.exceptions`](https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions). As an overview:\n\n- [`AgentsException`](https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.AgentsException \"AgentsException\") is the base class for all exceptions raised in the SDK.\n- [`MaxTurnsExceeded`](https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.MaxTurnsExceeded \"MaxTurnsExceeded\") is raised when the run exceeds the `max_turns` passed to the run methods.\n- [`ModelBehaviorError`](https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.ModelBehaviorError \"ModelBehaviorError\") is raised when the model produces invalid outputs, e.g. malformed JSON or using non-existent tools.\n- [`UserError`](https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.UserError \"UserError\") is raised when you (the person writing code using the SDK) make an error using the SDK.\n- [`InputGuardrailTripwireTriggered`](https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.InputGuardrailTripwireTriggered \"InputGuardrailTripwireTriggered\"), [`OutputGuardrailTripwireTriggered`](https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.OutputGuardrailTripwireTriggered \"OutputGuardrailTripwireTriggered\") is raised when a [guardrail](https://openai.github.io/openai-agents-python/guardrails/) is tripped.",
      "summary": "This page explains how to run agents using the Runner class in the OpenAI Agents Python SDK. It covers different run methods (async, sync, streaming), the agent execution loop, configuration options, streaming events, conversation management, and the main exceptions that may be raised during agent runs.",
      "similarity": 0.422626862212797,
      "path": "running_agents/",
      "language": "en",
      "keywords_array": [
        "Runner",
        "RunResult",
        "RunResultStreaming",
        "LLM",
        "run_config",
        "input_guardrails",
        "handoff_input_filter",
        "tracing",
        "tool calls",
        "AgentsException"
      ],
      "urls_array": [
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.output_guardrails",
        "https://openai.github.io/openai-agents-python/ref/handoffs/#agents.handoffs.Handoff.input_filter",
        "https://openai.github.io/openai-agents-python/guardrails/",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.Runner.run_streamed",
        "https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.AgentsException",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.Runner.run",
        "https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions",
        "https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.to_input_list",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.tracing_disabled",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.model_settings",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.trace_id",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.group_id",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.handoff_input_filter",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.model",
        "https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultStreaming",
        "https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.ModelBehaviorError",
        "https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.MaxTurnsExceeded",
        "https://openai.github.io/openai-agents-python/running_agents/#running-agents",
        "https://openai.github.io/openai-agents-python/streaming/",
        "https://openai.github.io/openai-agents-python/results/",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.Runner.run_sync",
        "https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.UserError",
        "https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResult",
        "https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.OutputGuardrailTripwireTriggered",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.workflow_name",
        "https://openai.github.io/openai-agents-python/tracing/",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.model_provider",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.Runner",
        "https://openai.github.io/openai-agents-python/ref/exceptions/#agents.exceptions.InputGuardrailTripwireTriggered",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.input_guardrails",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.trace_include_sensitive_data",
        "https://openai.github.io/openai-agents-python/ref/run/#agents.run.RunConfig.trace_metadata"
      ],
      "is_api_ref": false
    },
    {
      "id": "bee95aa5-b927-4bcb-af4a-d2aa1e387399",
      "title": "Models - OpenAI Agents SDK",
      "version": "79d84dbc-232f-47cd-8c7d-bcd990e1f838",
      "markdown_content": "[Skip to content](https://openai.github.io/openai-agents-python/models/#models)\n\n# Models\n\nThe Agents SDK comes with out-of-the-box support for OpenAI models in two flavors:\n\n- **Recommended**: the [`OpenAIResponsesModel`](https://openai.github.io/openai-agents-python/ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel \"OpenAIResponsesModel\"), which calls OpenAI APIs using the new [Responses API](https://platform.openai.com/docs/api-reference/responses).\n- The [`OpenAIChatCompletionsModel`](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel \"OpenAIChatCompletionsModel\"), which calls OpenAI APIs using the [Chat Completions API](https://platform.openai.com/docs/api-reference/chat).\n\n## Non-OpenAI models\n\nYou can use most other non-OpenAI models via the [LiteLLM integration](https://openai.github.io/openai-agents-python/models/litellm/). First, install the litellm dependency group:\n\n```md-code__content\npip install \"openai-agents[litellm]\"\n\n```\n\nThen, use any of the [supported models](https://docs.litellm.ai/docs/providers) with the `litellm/` prefix:\n\n```md-code__content\nclaude_agent = Agent(model=\"litellm/anthropic/claude-3-5-sonnet-20240620\", ...)\ngemini_agent = Agent(model=\"litellm/gemini/gemini-2.5-flash-preview-04-17\", ...)\n\n```\n\n### Other ways to use non-OpenAI models\n\nYou can integrate other LLM providers in 3 more ways (examples [here](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/)):\n\n1. [`set_default_openai_client`](https://openai.github.io/openai-agents-python/ref/#agents.set_default_openai_client \"set_default_openai_client\") is useful in cases where you want to globally use an instance of `AsyncOpenAI` as the LLM client. This is for cases where the LLM provider has an OpenAI compatible API endpoint, and you can set the `base_url` and `api_key`. See a configurable example in [examples/model\\_providers/custom\\_example\\_global.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py).\n2. [`ModelProvider`](https://openai.github.io/openai-agents-python/ref/models/interface/#agents.models.interface.ModelProvider \"ModelProvider\") is at the `Runner.run` level. This lets you say \"use a custom model provider for all agents in this run\". See a configurable example in [examples/model\\_providers/custom\\_example\\_provider.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py).\n3. [`Agent.model`](https://openai.github.io/openai-agents-python/ref/agent/#agents.agent.Agent.model \"model            class-attribute       instance-attribute   \") lets you specify the model on a specific Agent instance. This enables you to mix and match different providers for different agents. See a configurable example in [examples/model\\_providers/custom\\_example\\_agent.py](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py). An easy way to use most available models is via the [LiteLLM integration](https://openai.github.io/openai-agents-python/models/litellm/).\n\nIn cases where you do not have an API key from `platform.openai.com`, we recommend disabling tracing via `set_tracing_disabled()`, or setting up a [different tracing processor](https://openai.github.io/openai-agents-python/tracing/).\n\nNote\n\nIn these examples, we use the Chat Completions API/model, because most LLM providers don't yet support the Responses API. If your LLM provider does support it, we recommend using Responses.\n\n## Mixing and matching models\n\nWithin a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks. When configuring an [`Agent`](https://openai.github.io/openai-agents-python/ref/agent/#agents.agent.Agent \"Agent            dataclass   \"), you can select a specific model by either:\n\n1. Passing the name of a model.\n2. Passing any model name + a [`ModelProvider`](https://openai.github.io/openai-agents-python/ref/models/interface/#agents.models.interface.ModelProvider \"ModelProvider\") that can map that name to a Model instance.\n3. Directly providing a [`Model`](https://openai.github.io/openai-agents-python/ref/models/interface/#agents.models.interface.Model \"Model\") implementation.\n\nNote\n\nWhile our SDK supports both the [`OpenAIResponsesModel`](https://openai.github.io/openai-agents-python/ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel \"OpenAIResponsesModel\") and the [`OpenAIChatCompletionsModel`](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel \"OpenAIChatCompletionsModel\") shapes, we recommend using a single model shape for each workflow because the two shapes support a different set of features and tools. If your workflow requires mixing and matching model shapes, make sure that all the features you're using are available on both.\n\n```md-code__content\nfrom agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel\nimport asyncio\n\nspanish_agent = Agent(\n    name=\"Spanish agent\",\n    instructions=\"You only speak Spanish.\",\n    model=\"o3-mini\",\n)\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=OpenAIChatCompletionsModel(\n        model=\"gpt-4o\",\n        openai_client=AsyncOpenAI()\n    ),\n)\n\ntriage_agent = Agent(\n    name=\"Triage agent\",\n    instructions=\"Handoff to the appropriate agent based on the language of the request.\",\n    handoffs=[spanish_agent, english_agent],\n    model=\"gpt-3.5-turbo\",\n)\n\nasync def main():\n    result = await Runner.run(triage_agent, input=\"Hola, ¿cómo estás?\")\n    print(result.final_output)\n\n```\n\nWhen you want to further configure the model used for an agent, you can pass [`ModelSettings`](https://openai.github.io/openai-agents-python/ref/model_settings/#agents.model_settings.ModelSettings \"ModelSettings            dataclass   \"), which provides optional model configuration parameters such as temperature.\n\n```md-code__content\nfrom agents import Agent, ModelSettings\n\nenglish_agent = Agent(\n    name=\"English agent\",\n    instructions=\"You only speak English\",\n    model=\"gpt-4o\",\n    model_settings=ModelSettings(temperature=0.1),\n)\n\n```\n\n## Common issues with using other LLM providers\n\n### Tracing client error 401\n\nIf you get errors related to tracing, this is because traces are uploaded to OpenAI servers, and you don't have an OpenAI API key. You have three options to resolve this:\n\n1. Disable tracing entirely: [`set_tracing_disabled(True)`](https://openai.github.io/openai-agents-python/ref/#agents.set_tracing_disabled \"set_tracing_disabled\").\n2. Set an OpenAI key for tracing: [`set_tracing_export_api_key(...)`](https://openai.github.io/openai-agents-python/ref/#agents.set_tracing_export_api_key \"set_tracing_export_api_key\"). This API key will only be used for uploading traces, and must be from [platform.openai.com](https://platform.openai.com/).\n3. Use a non-OpenAI trace processor. See the [tracing docs](https://openai.github.io/openai-agents-python/tracing/#custom-tracing-processors).\n\n### Responses API support\n\nThe SDK uses the Responses API by default, but most other LLM providers don't yet support it. You may see 404s or similar issues as a result. To resolve, you have two options:\n\n1. Call [`set_default_openai_api(\"chat_completions\")`](https://openai.github.io/openai-agents-python/ref/#agents.set_default_openai_api \"set_default_openai_api\"). This works if you are setting `OPENAI_API_KEY` and `OPENAI_BASE_URL` via environment vars.\n2. Use [`OpenAIChatCompletionsModel`](https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel \"OpenAIChatCompletionsModel\"). There are examples [here](https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/).\n\n### Structured outputs support\n\nSome model providers don't have support for [structured outputs](https://platform.openai.com/docs/guides/structured-outputs). This sometimes results in an error that looks something like this:\n\n```md-code__content\nBadRequestError: Error code: 400 - {'error': {'message': \"'response_format.type' : value is not one of the allowed values ['text','json_object']\", 'type': 'invalid_request_error'}}\n\n```\n\nThis is a shortcoming of some model providers - they support JSON outputs, but don't allow you to specify the `json_schema` to use for the output. We are working on a fix for this, but we suggest relying on providers that do have support for JSON schema output, because otherwise your app will often break because of malformed JSON.\n\n## Mixing models across providers\n\nYou need to be aware of feature differences between model providers, or you may run into errors. For example, OpenAI supports structured outputs, multimodal input, and hosted file search and web search, but many other providers don't support these features. Be aware of these limitations:\n\n- Don't send unsupported `tools` to providers that don't understand them\n- Filter out multimodal inputs before calling models that are text-only\n- Be aware that providers that don't support structured JSON outputs will occasionally produce invalid JSON.",
      "summary": "This page explains how to use different language models with the Agents SDK, including OpenAI models (via Responses and Chat Completions APIs) and non-OpenAI models (using LiteLLM or custom integrations). It covers configuring models per agent, mixing models, common issues (like tracing and output formats), and provider-specific feature differences.",
      "similarity": 0.402071475982666,
      "path": "models/models/",
      "language": "en",
      "keywords_array": [
        "OpenAIResponsesModel",
        "OpenAIChatCompletionsModel",
        "LiteLLM integration",
        "ModelProvider",
        "Agent model",
        "AsyncOpenAI",
        "ModelSettings",
        "structured outputs",
        "tracing",
        "multimodal input"
      ],
      "urls_array": [
        "https://openai.github.io/openai-agents-python/ref/model_settings/#agents.model_settings.ModelSettings",
        "https://openai.github.io/openai-agents-python/ref/models/openai_responses/#agents.models.openai_responses.OpenAIResponsesModel",
        "https://platform.openai.com/",
        "https://platform.openai.com/docs/guides/structured-outputs",
        "https://openai.github.io/openai-agents-python/models/#models",
        "https://openai.github.io/openai-agents-python/ref/agent/#agents.agent.Agent.model",
        "https://openai.github.io/openai-agents-python/ref/#agents.set_default_openai_api",
        "https://platform.openai.com/docs/api-reference/responses",
        "https://openai.github.io/openai-agents-python/ref/#agents.set_default_openai_client",
        "https://openai.github.io/openai-agents-python/models/litellm/",
        "https://openai.github.io/openai-agents-python/tracing/#custom-tracing-processors",
        "https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_global.py",
        "https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_provider.py",
        "https://openai.github.io/openai-agents-python/ref/#agents.set_tracing_disabled",
        "https://platform.openai.com/docs/api-reference/chat",
        "https://openai.github.io/openai-agents-python/ref/models/interface/#agents.models.interface.Model",
        "https://openai.github.io/openai-agents-python/ref/models/openai_chatcompletions/#agents.models.openai_chatcompletions.OpenAIChatCompletionsModel",
        "https://openai.github.io/openai-agents-python/ref/models/interface/#agents.models.interface.ModelProvider",
        "https://openai.github.io/openai-agents-python/tracing/",
        "https://openai.github.io/openai-agents-python/ref/#agents.set_tracing_export_api_key",
        "https://docs.litellm.ai/docs/providers",
        "https://openai.github.io/openai-agents-python/ref/agent/#agents.agent.Agent",
        "https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/custom_example_agent.py",
        "https://github.com/openai/openai-agents-python/tree/main/examples/model_providers/"
      ],
      "is_api_ref": false
    }
  ]
}